# Data Document — MongoDB Adapter

> **Package:** `pyfly.data.document.mongodb`
> **Commons:** [`pyfly.data`](data.md) — shared ports, pagination, query parsing, entity mapping
>
> This guide covers the **MongoDB adapter** for document databases (Beanie ODM + Motor). For generic data concepts shared across all adapters (repository ports, `Page`/`Pageable`/`Sort`, `QueryMethodParser`, `Mapper`, extensibility), see the [Data Module Guide](data.md). For relational databases, see the [Data Relational Guide](data-relational.md).
>
> **Hexagonal by design:** your services depend on [`RepositoryPort[T, ID]`](data.md#repository-ports) (the port), never on `MongoRepository[T, ID]` (the adapter). MongoDB is the default document adapter today — but the layer is designed so any document backend (DynamoDB, Elasticsearch, etc.) can be added by implementing the same ports. See [Extending PyFly Data](data.md#extending-pyfly-data).

PyFly Data Document provides a document-oriented data access layer that implements the same Repository pattern and derived query method convention as the relational adapter — backed by MongoDB via Beanie ODM and Motor.

---

## Table of Contents

- [Architecture Overview](#architecture-overview)
- [Document Definition](#document-definition)
  - [BaseDocument](#basedocument)
  - [Audit Trail Fields](#audit-trail-fields)
  - [Settings Class](#settings-class)
  - [Indexed Fields](#indexed-fields)
  - [Defining Your Own Documents](#defining-your-own-documents)
- [MongoRepository\[T, ID\]](#mongorepositoryt-id)
  - [Creating a Repository](#creating-a-repository)
  - [CRUD Methods Reference](#crud-methods-reference)
- [Derived Query Methods](#derived-query-methods)
  - [How It Works](#how-it-works)
  - [MongoQueryMethodCompiler Operator Mapping](#mongoquerymethodcompiler-operator-mapping)
  - [Connectors](#connectors)
  - [Ordering](#ordering)
  - [Complete Derived Query Examples](#complete-derived-query-examples)
- [Custom Queries with @query](#custom-queries-with-query)
  - [Find Filter Queries](#find-filter-queries)
  - [Aggregation Pipelines](#aggregation-pipelines)
  - [Parameter Substitution](#parameter-substitution)
  - [MongoQueryExecutor Internals](#mongoqueryexecutor-internals)
- [Configuration](#configuration)
  - [DocumentProperties](#documentproperties)
  - [pyfly.yaml Keys](#pyflyyaml-keys)
  - [Environment Variables](#environment-variables)
- [Auto-Configuration](#auto-configuration)
  - [Detection Flow](#detection-flow)
  - [Beanie Initialization](#beanie-initialization)
  - [Document Class Discovery](#document-class-discovery)
- [Transaction Management](#transaction-management)
  - [mongo_transactional Decorator](#mongo_transactional-decorator)
  - [Replica Set Requirement](#replica-set-requirement)
  - [Usage Example](#usage-example)
- [MongoRepositoryBeanPostProcessor](#mongorepositorybeanbeanpostprocessor)
  - [How It Works](#how-it-works-1)
  - [Stub Detection](#stub-detection)
- [Pagination](#pagination)
  - [Paginated Queries](#paginated-queries)
  - [Sort Specification Building](#sort-specification-building)
- [Integration with Web Layer](#integration-with-web-layer)
  - [Controller with Valid\[T\] and MongoRepository](#controller-with-validt-and-mongorepository)
- [Complete CRUD Example](#complete-crud-example)
- [See Also](#see-also)

---

## Architecture Overview

All concrete types live in the MongoDB adapter package. The namespace `pyfly.data.document` is a pass-through and does not re-export anything.

```python
from pyfly.data.document.mongodb import (
    BaseDocument,
    MongoRepository,
    MongoQueryMethodCompiler,
    MongoRepositoryBeanPostProcessor,
    mongo_transactional,
)
```

> **Note:** Always import concrete types from `pyfly.data.document.mongodb`. Commons types (`Page`, `Pageable`, `RepositoryPort`, etc.) are imported from `pyfly.data` — see the [Data Module Guide](data.md#import-rules).

Source files:
- `src/pyfly/data/__init__.py` — commons layer (Page, Pageable, ports)
- `src/pyfly/data/document/mongodb/__init__.py` — MongoDB adapter package exports

---

## Document Definition

### BaseDocument

`BaseDocument` is the base class for all MongoDB documents in a PyFly application. It extends `beanie.Document` and provides audit trail fields that are automatically populated on insert and update.

```python
from pyfly.data.document.mongodb import BaseDocument
```

All domain documents should inherit from `BaseDocument` to gain the automatic audit trail, just as all relational entities inherit from `BaseEntity` in the SQLAlchemy adapter.

### Audit Trail Fields

`BaseDocument` provides four audit trail fields in addition to the `id` field inherited from `beanie.Document`:

| Field        | Type              | Default                         | Description                          |
|--------------|-------------------|---------------------------------|--------------------------------------|
| `id`         | `PydanticObjectId`| Auto-generated by MongoDB       | Document primary key (ObjectId)      |
| `created_at` | `datetime`        | `datetime.now(UTC)` on creation | Timestamp when the document was created |
| `updated_at` | `datetime`        | `datetime.now(UTC)` on creation | Timestamp of the last update         |
| `created_by` | `str \| None`     | `None`                          | Creator identifier                   |
| `updated_by` | `str \| None`     | `None`                          | Last updater identifier              |

The `created_at` and `updated_at` fields use `pydantic.Field(default_factory=...)` with a lambda that calls `datetime.now(UTC)`, ensuring timezone-aware UTC timestamps.

The `BaseDocument` class also enables Beanie state management:

```python
class BaseDocument(Document):
    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(UTC))
    created_by: str | None = None
    updated_by: str | None = None

    class Settings:
        use_state_management = True
```

With `use_state_management = True`, Beanie tracks which fields have changed since the document was loaded, enabling efficient partial updates via `save_changes()`.

### Settings Class

Every Beanie document uses an inner `Settings` class to configure collection-level options. The most important setting is `name`, which defines the MongoDB collection name:

```python
class UserDocument(BaseDocument):
    name: str
    email: str

    class Settings:
        name = "users"
```

If you omit the `Settings` class or the `name` attribute, Beanie derives the collection name from the class name (e.g., `UserDocument` becomes `UserDocument` as the collection name). It is best practice to always set `name` explicitly for clarity and consistency.

Common `Settings` attributes:

| Attribute              | Type   | Description                                           |
|------------------------|--------|-------------------------------------------------------|
| `name`                 | `str`  | MongoDB collection name                               |
| `use_state_management` | `bool` | Track field changes for partial updates (inherited from `BaseDocument`) |
| `indexes`              | `list` | Additional Beanie index definitions                   |

### Indexed Fields

You can define indexes using Beanie's `Indexed` type or the `Settings.indexes` list. The `Indexed` type is the simplest approach for single-field indexes:

```python
from beanie import Indexed

class UserDocument(BaseDocument):
    name: str
    email: Indexed(str, unique=True)
    role: Indexed(str)

    class Settings:
        name = "users"
```

For compound indexes or more complex index configurations, use the `Settings.indexes` list:

```python
from pymongo import IndexModel, ASCENDING, DESCENDING

class OrderDocument(BaseDocument):
    customer_id: str
    status: str
    total: float
    region: str

    class Settings:
        name = "orders"
        indexes = [
            IndexModel(
                [("customer_id", ASCENDING), ("status", ASCENDING)],
                name="idx_customer_status",
            ),
            IndexModel(
                [("region", ASCENDING), ("total", DESCENDING)],
                name="idx_region_total",
            ),
        ]
```

Indexes are created automatically when Beanie initializes the document models during application startup.

### Defining Your Own Documents

Extend `BaseDocument` and declare your domain fields using standard Pydantic field declarations:

```python
from pyfly.data.document.mongodb import BaseDocument
from beanie import Indexed
from pydantic import Field


class ProductDocument(BaseDocument):
    name: str
    sku: Indexed(str, unique=True)
    description: str = ""
    price: float = Field(gt=0)
    category: str
    tags: list[str] = Field(default_factory=list)
    active: bool = True

    class Settings:
        name = "products"
```

This document will have all five inherited fields (`id`, `created_at`, `updated_at`, `created_by`, `updated_by`) plus your seven custom fields. Because `BaseDocument` extends `beanie.Document` (which extends `pydantic.BaseModel`), all Pydantic validation, serialization, and field configuration features are available.

Nested documents use standard Pydantic models:

```python
from pydantic import BaseModel


class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str


class CustomerDocument(BaseDocument):
    name: str
    email: str
    address: Address | None = None
    tags: list[str] = Field(default_factory=list)

    class Settings:
        name = "customers"
```

Source file: `src/pyfly/data/document/mongodb/document.py`

---

## MongoRepository[T, ID]

The `MongoRepository[T, ID]` class provides generic async CRUD operations for Beanie documents. It mirrors the `Repository[T, ID]` class from the SQLAlchemy adapter but operates against MongoDB. The two type parameters are:

- **T** — The document type (a `BaseDocument` subclass)
- **ID** — The primary key type (typically `PydanticObjectId` or `str`)

Unlike the SQLAlchemy `Repository`, `MongoRepository` does not require a session to be injected. Beanie uses a globally initialized Motor client, so the repository operates without session management.

When you subclass `MongoRepository[T, ID]` with concrete type parameters, the framework automatically extracts the document type and ID type via `__init_subclass__`. No explicit `__init__` is needed:

```python
from pyfly.data.document.mongodb import MongoRepository, BaseDocument
from pyfly.container import repository as repo_stereotype


@repo_stereotype
class ProductRepository(MongoRepository[ProductDocument, str]):
    pass

# Usage:
product = await repo.save(ProductDocument(name="Widget", price=9.99, category="gadgets"))
found = await repo.find_by_id(product.id)
```

`MongoRepository[T, ID]` satisfies the [`RepositoryPort[T, ID]`](data.md#repository-ports) protocol, enabling hexagonal architecture where your service layer depends on the port, not the adapter.

### Creating a Repository

Subclass `MongoRepository[T, ID]` with concrete type parameters and register it with the `@repository` stereotype:

```python
from pyfly.data.document.mongodb import MongoRepository
from pyfly.container import repository as repo_stereotype


@repo_stereotype
class ProductRepository(MongoRepository[ProductDocument, str]):
    pass
```

For documents with `PydanticObjectId` primary keys (the default):

```python
from beanie import PydanticObjectId


@repo_stereotype
class OrderRepository(MongoRepository[OrderDocument, PydanticObjectId]):
    pass
```

**How it works:**

1. `__init_subclass__` inspects `__orig_bases__` to extract the document type (`ProductDocument`) and ID type (`str`) from the generic parameters at class definition time.
2. Beanie handles the database connection globally through the Motor client initialized at startup by `BeanieInitializer`.
3. The document type is used internally for all query operations — no need to pass it manually.

### CRUD Methods Reference

| Method                                     | Return Type  | Description                                    |
|--------------------------------------------|--------------|------------------------------------------------|
| `save(entity)`                             | `T`          | Insert or update; calls `entity.save()`        |
| `find_by_id(id: ID)`                       | `T \| None`  | Find by primary key via `model.get(id)`        |
| `find_all(**filters)`                       | `list[T]`    | Find all, optionally filtered by field values  |
| `delete(id: ID)`                            | `None`       | Delete by primary key (no-op if not found)     |
| `count()`                                   | `int`        | Count all documents in the collection          |
| `exists(id: ID)`                            | `bool`       | Check if a document with this ID exists        |
| `find_paginated(page, size, pageable)`      | `Page[T]`    | Paginated query with optional sorting          |

**save()** calls Beanie's `entity.save()`, which performs an upsert — inserting the document if it is new or updating it if it already exists. The returned entity is the same object with any server-side defaults applied.

**find_all()** accepts keyword arguments that are translated into equality filters passed to Beanie's `find()`:

```python
orders = await repo.find_all(status="PENDING", customer_id="abc")
# Equivalent to: db.orders.find({"status": "PENDING", "customer_id": "abc"})
```

When called without filters, it returns all documents in the collection.

**delete()** looks up the entity by ID first, then calls `entity.delete()` if found. If not found, it is a no-op.

**find_paginated()** supports both simple page/size arguments and a `Pageable` object:

```python
# Simple pagination
page = await repo.find_paginated(page=1, size=20)

# With Pageable and sorting
from pyfly.data import Pageable, Sort

pageable = Pageable.of(page=1, size=20, sort=Sort.by("name"))
page = await repo.find_paginated(pageable=pageable)
```

Source file: `src/pyfly/data/document/mongodb/repository.py`

---

## Derived Query Methods

PyFly automatically generates MongoDB query implementations from method names using the same Spring Data naming convention as the relational adapter. You define stub methods on your repository, and the `MongoRepositoryBeanPostProcessor` compiles them into real MongoDB queries at startup.

For the full naming convention reference (prefixes, operators, connectors, ordering), see the [Data Module Guide — Derived Query Methods](data.md#derived-query-methods).

### How It Works

The derived query pipeline consists of two stages, split between the shared commons layer and the MongoDB adapter:

1. **Parsing (shared):** The `QueryMethodParser` (from `pyfly.data`) parses the method name into a `ParsedQuery` containing predicates, connectors, and order clauses. This parser is identical for both the SQLAlchemy and MongoDB adapters.

2. **Compilation (adapter-specific):** The `MongoQueryMethodCompiler` takes the `ParsedQuery` and compiles it into an async callable that builds a MongoDB filter document and executes it via Beanie.

### MongoQueryMethodCompiler Operator Mapping

The `MongoQueryMethodCompiler._build_clause()` method translates each parsed operator into a MongoDB filter expression:

| Operator       | Method Suffix          | MongoDB Filter Expression                    | Args |
|----------------|------------------------|----------------------------------------------|------|
| `eq`           | *(none)*               | `{field: value}`                             | 1    |
| `not`          | `_not`                 | `{field: {"$ne": value}}`                    | 1    |
| `gt`           | `_greater_than`        | `{field: {"$gt": value}}`                    | 1    |
| `gte`          | `_greater_than_equal`  | `{field: {"$gte": value}}`                   | 1    |
| `lt`           | `_less_than`           | `{field: {"$lt": value}}`                    | 1    |
| `lte`          | `_less_than_equal`     | `{field: {"$lte": value}}`                   | 1    |
| `between`      | `_between`             | `{field: {"$gte": low, "$lte": high}}`       | 2    |
| `like`         | `_like`                | `{field: {"$regex": pattern}}`               | 1    |
| `containing`   | `_containing`          | `{field: {"$regex": ".*value.*", "$options": "i"}}` | 1 |
| `in`           | `_in`                  | `{field: {"$in": values}}`                   | 1 (list) |
| `is_null`      | `_is_null`             | `{field: None}`                              | 0    |
| `is_not_null`  | `_is_not_null`         | `{field: {"$ne": None}}`                     | 0    |

Key notes on the MongoDB-specific behavior:

- **`containing`** uses a case-insensitive regex (`$options: "i"`) and wraps the value in `.*...*`. The value is regex-escaped to prevent injection.
- **`like`** translates SQL LIKE syntax: `%` becomes `.*` and `_` becomes `.` in the regex pattern. The rest of the value is regex-escaped.
- **`between`** consumes two arguments and combines them into a single filter with both `$gte` and `$lte`.
- **`is_null`** and **`is_not_null`** consume zero arguments.

### Connectors

Multiple predicates are connected with `_and_` or `_or_`, producing MongoDB's `$and` or `$or` operators:

```python
# AND: {"$and": [{"status": "ACTIVE"}, {"role": "admin"}]}
async def find_by_status_and_role(self, status: str, role: str) -> list[UserDocument]: ...

# OR: {"$or": [{"status": "ACTIVE"}, {"role": "admin"}]}
async def find_by_status_or_role(self, status: str, role: str) -> list[UserDocument]: ...
```

When all connectors are the same type, the compiler produces a flat `$and` or `$or` array. When connectors are mixed (both AND and OR in one method name), the compiler builds a nested expression evaluated left to right:

```python
# Mixed: find where status=? AND (role=? OR active=?)
# This is built as: {"$and": [{"$and": [{"status": ?}, {"role": ?}]}, {"active": ?}]}
async def find_by_status_and_role_or_active(
    self, status: str, role: str, active: bool
) -> list[UserDocument]: ...
```

For a single predicate with no connectors, the filter is a plain document (no `$and` wrapper):

```python
# Simple: {"email": value}
async def find_by_email(self, email: str) -> list[UserDocument]: ...
```

### Ordering

Append `_order_by_{field}_{asc|desc}` to control result ordering. The compiler translates these into pymongo sort specifications:

```python
# Sort by created_at descending: [("created_at", pymongo.DESCENDING)]
async def find_by_status_order_by_created_at_desc(
    self, status: str
) -> list[OrderDocument]: ...

# Multiple sort fields: [("name", ASCENDING), ("created_at", DESCENDING)]
async def find_by_active_order_by_name_asc_created_at_desc(
    self, active: bool
) -> list[UserDocument]: ...
```

### Complete Derived Query Examples

```python
@repo_stereotype
class OrderRepository(MongoRepository[OrderDocument, PydanticObjectId]):

    # Equals (default operator)
    # -> {"status": value}
    async def find_by_status(self, status: str) -> list[OrderDocument]: ...

    # Multiple conditions with AND
    # -> {"$and": [{"customer_id": value}, {"status": value}]}
    async def find_by_customer_id_and_status(
        self, customer_id: str, status: str
    ) -> list[OrderDocument]: ...

    # Greater than
    # -> {"total": {"$gt": value}}
    async def find_by_total_greater_than(self, min_total: float) -> list[OrderDocument]: ...

    # Between (takes 2 arguments)
    # -> {"total": {"$gte": low, "$lte": high}}
    async def find_by_total_between(self, low: float, high: float) -> list[OrderDocument]: ...

    # Contains (case-insensitive regex)
    # -> {"customer_id": {"$regex": ".*value.*", "$options": "i"}}
    async def find_by_customer_id_containing(self, fragment: str) -> list[OrderDocument]: ...

    # IN a list
    # -> {"status": {"$in": ["PENDING", "SHIPPED"]}}
    async def find_by_status_in(self, statuses: list[str]) -> list[OrderDocument]: ...

    # IS NULL (zero arguments consumed)
    # -> {"deleted_at": None}
    async def find_by_deleted_at_is_null(self) -> list[OrderDocument]: ...

    # IS NOT NULL
    # -> {"email": {"$ne": None}}
    async def find_by_email_is_not_null(self) -> list[OrderDocument]: ...

    # COUNT prefix
    # -> count({role: value})
    async def count_by_role(self, role: str) -> int: ...

    # EXISTS prefix
    # -> count({email: value}) > 0
    async def exists_by_email(self, email: str) -> bool: ...

    # DELETE prefix (returns number of deleted documents)
    # -> finds all matching, deletes each, returns count
    async def delete_by_status(self, status: str) -> int: ...

    # With ordering
    # -> find({"status": value}).sort([("created_at", DESCENDING)])
    async def find_by_status_order_by_created_at_desc(
        self, status: str
    ) -> list[OrderDocument]: ...

    # Complex: AND + ordering
    # -> find({"$and": [{status: ?}, {customer_id: ?}]}).sort([("total", DESCENDING)])
    async def find_by_status_and_customer_id_order_by_total_desc(
        self, status: str, customer_id: str
    ) -> list[OrderDocument]: ...
```

Each method body should be a stub (`...` or `pass`). The `MongoRepositoryBeanPostProcessor` detects them and replaces them with real implementations at startup.

Source file: `src/pyfly/data/document/mongodb/query_compiler.py`

---

## Custom Queries with @query

For queries that cannot be expressed through method naming conventions, the `@query` decorator lets you write MongoDB filter documents or aggregation pipelines directly as JSON strings with named parameter substitution.

```python
from pyfly.data.relational.sqlalchemy import query  # shared @query decorator
```

The `@query` decorator is shared between the relational and document adapters. For MongoDB, the `MongoQueryExecutor` compiles the decorated methods into async callables that execute against Beanie document models.

### Find Filter Queries

A query string that starts with `{` is treated as a MongoDB **find filter**:

```python
@repo_stereotype
class UserRepository(MongoRepository[UserDocument, str]):

    @query('{"email": ":email"}')
    async def find_by_email_exact(self, email: str) -> list[UserDocument]: ...

    @query('{"active": true, "role": ":role"}')
    async def find_active_by_role(self, role: str) -> list[UserDocument]: ...

    @query('{"age": {"$gte": ":min_age", "$lte": ":max_age"}}')
    async def find_by_age_range(self, min_age: int, max_age: int) -> list[UserDocument]: ...
```

The compiled query calls `model.find(filter_doc).to_list()` and returns `list[entity]`.

### Aggregation Pipelines

A query string that starts with `[` is treated as a MongoDB **aggregation pipeline**:

```python
@repo_stereotype
class OrderRepository(MongoRepository[OrderDocument, str]):

    @query('[{"$match": {"status": ":status"}}, {"$group": {"_id": "$category", "total": {"$sum": "$amount"}}}]')
    async def total_by_category(self, status: str) -> list[dict]: ...

    @query('[{"$match": {"customer_id": ":customer_id"}}, {"$sort": {"created_at": -1}}, {"$limit": 10}]')
    async def recent_orders(self, customer_id: str) -> list[dict]: ...
```

Aggregation pipeline queries use the underlying Motor collection directly (via `get_pymongo_collection()`) and return `list[dict]` rather than document instances.

### Parameter Substitution

Named parameters use the `:param_name` convention inside JSON string values. During execution, the `MongoQueryExecutor` substitutes parameter values while preserving Python types:

| Placeholder in JSON | Method Parameter | Substituted Value |
|---|---|---|
| `":email"` (exact match) | `email="alice@example.com"` | `"alice@example.com"` (str) |
| `":min_age"` (exact match) | `min_age=18` | `18` (int, not string) |
| `":active"` (exact match) | `active=True` | `True` (bool) |
| `"prefix_:name_suffix"` (embedded) | `name="alice"` | `"prefix_alice_suffix"` (str interpolation) |

**Substitution rules:**

- If the entire JSON string value is a single `:param_name` placeholder, it is replaced by the actual Python value, preserving the type (int, bool, list, etc.).
- If `:param_name` appears within a larger string, it is replaced via string interpolation with `str(value)`.
- Dicts and lists are recursed into.
- Non-string values (int, float, bool, None) pass through unchanged.

```python
# The filter {"age": ":min_age"} with min_age=18
# becomes {"age": 18}  (int, not "18")

# The filter {"name": {"$regex": ".*:pattern.*"}} with pattern="alice"
# becomes {"name": {"$regex": ".*alice.*"}}  (string interpolation)
```

### MongoQueryExecutor Internals

The `MongoQueryExecutor` is used by the `MongoRepositoryBeanPostProcessor` to compile `@query`-decorated methods at startup:

1. **Validation:** Checks that the method has a `__pyfly_query__` attribute (set by the `@query` decorator).
2. **JSON parsing:** Parses the query string once at compile time to validate it and detect whether it is a find filter (JSON object) or an aggregation pipeline (JSON array).
3. **Template compilation:** Stores the parsed template and creates an async callable that substitutes parameters at execution time.

```python
from pyfly.data.document.mongodb.query import MongoQueryExecutor

executor = MongoQueryExecutor()
compiled_fn = executor.compile_query_method(method, entity_type)
# compiled_fn(model=UserDocument, email="alice@example.com") -> list[UserDocument]
```

**Source:** `src/pyfly/data/document/mongodb/query.py`

---

## Configuration

### DocumentProperties

The `DocumentProperties` dataclass captures all document database configuration under the `pyfly.data.document.*` namespace:

```python
from pyfly.core.config import config_properties
from dataclasses import dataclass


@config_properties(prefix="pyfly.data.document")
@dataclass
class DocumentProperties:
    enabled: bool = False
    uri: str = "mongodb://localhost:27017"
    database: str = "pyfly"
    min_pool_size: int = 0
    max_pool_size: int = 100
```

| Field           | Type   | Default                      | Description                                      |
|-----------------|--------|------------------------------|--------------------------------------------------|
| `enabled`       | `bool` | `False`                      | Enable the MongoDB subsystem                     |
| `uri`           | `str`  | `"mongodb://localhost:27017"`| MongoDB connection URI                           |
| `database`      | `str`  | `"pyfly"`                    | Database name to connect to                      |
| `min_pool_size` | `int`  | `0`                          | Minimum number of connections in the Motor pool  |
| `max_pool_size` | `int`  | `100`                        | Maximum number of connections in the Motor pool  |

### pyfly.yaml Keys

Configure MongoDB in your `pyfly.yaml` (or `application.yml`):

```yaml
pyfly:
  data:
    document:
      enabled: true
      uri: mongodb://localhost:27017
      database: my_app
      min_pool_size: 5
      max_pool_size: 50
```

For a MongoDB Atlas connection:

```yaml
pyfly:
  data:
    document:
      enabled: true
      uri: mongodb+srv://user:password@cluster.mongodb.net/?retryWrites=true&w=majority
      database: production_db
      min_pool_size: 10
      max_pool_size: 100
```

For a replica set deployment (required for transactions):

```yaml
pyfly:
  data:
    document:
      enabled: true
      uri: mongodb://mongo1:27017,mongo2:27017,mongo3:27017/?replicaSet=rs0
      database: my_app
      min_pool_size: 5
      max_pool_size: 50
```

### Environment Variables

Following PyFly's configuration resolution order, you can override any MongoDB property with environment variables. The pattern is the YAML key path with dots replaced by underscores and uppercased:

| Environment Variable       | Overrides                   | Example                          |
|----------------------------|-----------------------------|----------------------------------|
| `PYFLY_DATA_DOCUMENT_ENABLED`   | `pyfly.data.document.enabled`    | `true`                           |
| `PYFLY_DATA_DOCUMENT_URI`       | `pyfly.data.document.uri`        | `mongodb://prod-host:27017`      |
| `PYFLY_DATA_DOCUMENT_DATABASE`  | `pyfly.data.document.database`   | `production_db`                  |
| `PYFLY_DATA_DOCUMENT_MIN_POOL_SIZE` | `pyfly.data.document.min_pool_size` | `10`                       |
| `PYFLY_DATA_DOCUMENT_MAX_POOL_SIZE` | `pyfly.data.document.max_pool_size` | `200`                      |

This is useful for containerized deployments where secrets and connection strings are injected via environment:

```bash
export PYFLY_DATA_DOCUMENT_ENABLED=true
export PYFLY_DATA_DOCUMENT_URI="mongodb+srv://user:secret@cluster.mongodb.net"
export PYFLY_DATA_DOCUMENT_DATABASE=production_db
```

Source file: `src/pyfly/config/properties/mongodb.py` (class `DocumentProperties`)

---

## Auto-Configuration

PyFly uses a decentralized, config-driven auto-configuration system to detect and wire the MongoDB adapter at startup. Each subsystem provides its own `@auto_configuration` class that is discovered via `pyfly.auto_configuration` entry points. This mirrors the Spring Boot auto-configuration pattern.

### Detection Flow

The `DocumentAutoConfiguration` class (in `src/pyfly/data/document/auto_configuration.py`) is decorated with conditional annotations that control whether the MongoDB subsystem is activated:

1. **`@auto_configuration`** — Marks the class for discovery via the `pyfly.auto_configuration` entry-point group.

2. **`@conditional_on_class("beanie")`** — The class is skipped entirely if the `beanie` package is not importable. This is the library-availability gate.

3. **`@conditional_on_property("pyfly.data.document.enabled", having_value="true")`** — The class is skipped if the property is absent or not `"true"`. This is the opt-in gate.

When both conditions are satisfied, `DocumentAutoConfiguration` registers three beans via `@bean` methods:

```python
@auto_configuration
@conditional_on_class("beanie")
@conditional_on_property("pyfly.data.document.enabled", having_value="true")
class DocumentAutoConfiguration:

    @bean
    def motor_client(self, config: Config) -> AsyncIOMotorClient:
        uri = str(config.get("pyfly.data.document.uri", "mongodb://localhost:27017"))
        return AsyncIOMotorClient(uri)

    @bean
    def mongo_post_processor(self) -> MongoRepositoryBeanPostProcessor:
        return MongoRepositoryBeanPostProcessor()

    @bean
    def odm_initializer(
        self, config: Config, container: Container, motor_client: AsyncIOMotorClient,
    ) -> BeanieInitializer:
        return BeanieInitializer(motor_client=motor_client, config=config, container=container)
```

The detection is purely library-based: if Beanie is installed in your Python environment, the MongoDB adapter is available. You still need to set `pyfly.data.document.enabled: true` in config to activate it.

### Beanie Initialization

Beanie requires explicit initialization before any document operations can be performed. The `BeanieInitializer` lifecycle bean (in `src/pyfly/data/document/mongodb/initializer.py`) handles this automatically during application startup.

`BeanieInitializer` is registered as a bean by `DocumentAutoConfiguration.odm_initializer()`. Because it implements the `start()`/`stop()` lifecycle protocol, `ApplicationContext._start_infrastructure()` calls it during the startup sequence:

**`start()`:**
1. Reads `pyfly.data.document.database` from config (defaults to `"pyfly"`).
2. Scans the container for all registered `BaseDocument` subclasses.
3. Calls Beanie's `init_beanie()` with the Motor database and discovered document models.

**`stop()`:**
1. Closes the Motor client, releasing all pooled connections.

You do not need to call any initialization function manually. As long as `pyfly.data.document.enabled: true` is set and `beanie` is installed, the `BeanieInitializer` is created and started automatically.

### Document Class Discovery

Unlike manual initialization where you must enumerate all document models, `BeanieInitializer.start()` discovers them automatically using a two-phase scan:

1. **Primary: Repository-based discovery** — Scans all registered `MongoRepository` subclasses and reads their `_entity_type` attribute (set by `__init_subclass__`). This is the primary mechanism because document models are typically referenced by repositories, not registered directly as beans.

2. **Secondary: Direct registration** — Scans for any `BaseDocument` subclasses registered directly in the container (e.g., standalone models not associated with a repository).

```python
async def start(self) -> None:
    from pyfly.data.document.mongodb.document import BaseDocument
    from pyfly.data.document.mongodb.repository import MongoRepository

    db_name = str(self._config.get("pyfly.data.document.database", "pyfly"))

    document_models: list[type] = []

    # Primary: discover from MongoRepository._entity_type (set by __init_subclass__)
    for cls in self._container._registrations:
        if (
            isinstance(cls, type)
            and issubclass(cls, MongoRepository)
            and cls is not MongoRepository
        ):
            entity_type = getattr(cls, "_entity_type", None)
            if (
                entity_type is not None
                and isinstance(entity_type, type)
                and issubclass(entity_type, BaseDocument)
                and entity_type not in document_models
            ):
                document_models.append(entity_type)

    # Secondary: directly registered BaseDocument subclasses (e.g. standalone models)
    for cls in self._container._registrations:
        if (
            isinstance(cls, type)
            and issubclass(cls, BaseDocument)
            and cls is not BaseDocument
            and cls not in document_models
        ):
            document_models.append(cls)

    if document_models:
        from beanie import init_beanie
        await init_beanie(database=self._motor_client[db_name], document_models=document_models)
```

This means you only need to define a `MongoRepository[MyDocument, ID]` subclass — the document model is discovered automatically from the generic type parameter. There is no need to register document models as beans or maintain a manual list.

Source files:
- `src/pyfly/data/document/auto_configuration.py` — `DocumentAutoConfiguration` (Motor client, post-processor, and initializer beans)
- `src/pyfly/data/document/mongodb/initializer.py` — `BeanieInitializer` (lifecycle bean)

---

## Transaction Management

### mongo_transactional Decorator

The `@mongo_transactional` decorator provides declarative async transaction management for MongoDB, mirroring the `@reactive_transactional` decorator from the SQLAlchemy adapter:

```python
from pyfly.data.document.mongodb import mongo_transactional
from motor.motor_asyncio import AsyncIOMotorClient

client: AsyncIOMotorClient = ...


@mongo_transactional(client)
async def transfer_funds(from_id: str, to_id: str, amount: float) -> None:
    from_account = await AccountDocument.get(from_id)
    to_account = await AccountDocument.get(to_id)

    from_account.balance -= amount
    to_account.balance += amount

    await from_account.save()
    await to_account.save()
    # Transaction is committed automatically on success
    # Transaction is aborted automatically on exception
```

**How it works:**

1. Starts a new Motor session via `client.start_session()`.
2. Begins a transaction on the session via `session.start_transaction()`.
3. Calls the wrapped function with all original arguments.
4. On success: the transaction is committed (via the `async with` context manager).
5. On exception: the transaction is aborted and the exception is re-raised.

Unlike `@reactive_transactional` (which injects the session as the first argument), `@mongo_transactional` does not inject the session. The Beanie document operations automatically participate in the active transaction through Motor's session context.

### Replica Set Requirement

MongoDB transactions require a replica set deployment. Standalone MongoDB instances do not support multi-document transactions. If you attempt to use `@mongo_transactional` against a standalone instance, MongoDB will raise an error.

For local development, you can run a single-node replica set:

```bash
# Start MongoDB as a single-node replica set
mongod --replSet rs0 --bind_ip localhost --port 27017

# Initialize the replica set (run once in mongosh)
rs.initiate({_id: "rs0", members: [{_id: 0, host: "localhost:27017"}]})
```

Or use Docker Compose:

```yaml
version: "3.8"
services:
  mongo:
    image: mongo:7
    command: ["--replSet", "rs0", "--bind_ip_all"]
    ports:
      - "27017:27017"
    healthcheck:
      test: |
        mongosh --eval 'try { rs.status() } catch { rs.initiate({_id:"rs0",members:[{_id:0,host:"localhost:27017"}]}) }'
      interval: 10s
      start_period: 30s
```

### Usage Example

A complete example showing transactional order processing. The `motor_client` bean is provided automatically by `DocumentAutoConfiguration` and can be injected into your service:

```python
from pyfly.data.document.mongodb import mongo_transactional
from motor.motor_asyncio import AsyncIOMotorClient

# The Motor client is auto-configured by DocumentAutoConfiguration.
# Inject it into your service via the container.
client: AsyncIOMotorClient = ...


@mongo_transactional(client)
async def place_order(customer_id: str, product_id: str, quantity: int) -> OrderDocument:
    """Place an order and decrement inventory atomically."""
    # Decrement inventory
    inventory = await InventoryDocument.find_one(
        InventoryDocument.product_id == product_id
    )
    if inventory is None or inventory.quantity < quantity:
        raise ValueError("Insufficient inventory")

    inventory.quantity -= quantity
    await inventory.save()

    # Create order
    order = OrderDocument(
        customer_id=customer_id,
        product_id=product_id,
        quantity=quantity,
        status="CONFIRMED",
    )
    await order.save()
    return order
    # Both operations commit together, or both are rolled back
```

Source file: `src/pyfly/data/document/mongodb/transactional.py`

---

## MongoRepositoryBeanPostProcessor

The `MongoRepositoryBeanPostProcessor` is a `BeanPostProcessor` that runs after each repository bean is initialized. It scans the repository class for stub methods and replaces them with real MongoDB query implementations. It mirrors the `RepositoryBeanPostProcessor` from the SQLAlchemy adapter but targets MongoDB via Beanie ODM.

### How It Works

The `after_init(bean, bean_name)` method:

1. **Checks the bean type.** If the bean is not an instance of `MongoRepository`, it is returned unchanged.

2. **Identifies custom methods.** Iterates over all attributes defined on the bean's class (excluding private attributes starting with `_` and methods inherited from the base `MongoRepository`).

3. **Detects derived query methods.** For each method that starts with a recognized prefix (`find_by_`, `count_by_`, `exists_by_`, `delete_by_`) and is a stub, the processor:
   - Parses the method name via `QueryMethodParser.parse()`.
   - Compiles the parsed query via `MongoQueryMethodCompiler.compile()`.
   - Wraps the compiled function to inject `bean._model`.
   - Replaces the stub method on the bean instance.

### Stub Detection

A method is considered a stub when its code object contains no meaningful constants beyond `None` and `Ellipsis`. This covers both forms:

```python
async def find_by_status(self, status: str) -> list[OrderDocument]: ...    # Ellipsis stub
async def find_by_status(self, status: str) -> list[OrderDocument]: pass   # Pass stub
```

The post-processor is registered automatically by `DocumentAutoConfiguration.mongo_post_processor()` when the MongoDB subsystem is enabled. No manual registration is required.

Source file: `src/pyfly/data/document/mongodb/post_processor.py`

---

## Pagination

For the full `Pageable`, `Sort`, `Order`, and `Page[T]` API reference, see the [Data Module Guide — Pagination & Sorting](data.md#pagination--sorting).

### Paginated Queries

The `find_paginated()` method on `MongoRepository` supports both simple page/size arguments and a full `Pageable` object with sorting:

```python
from pyfly.data import Pageable, Sort

# Basic pagination (page 1, 20 items per page)
page = await repo.find_paginated(page=1, size=20)

# With Pageable (overrides page/size and adds sorting)
pageable = Pageable.of(
    page=2,
    size=10,
    sort=Sort.by("created_at").descending(),
)
page = await repo.find_paginated(pageable=pageable)
```

When a `pageable` is provided, its `page`, `size`, and `sort` override the primitive `page` and `size` arguments.

The implementation:
1. Counts total documents in the collection via `find_all().count()`.
2. Calculates the offset: `(page - 1) * size`.
3. Builds a sort specification from the `Pageable.sort` orders (if provided).
4. Applies `.sort()`, `.skip()`, and `.limit()` to the Beanie query.
5. Returns a `Page[T]` with the items, total count, page number, and size.

### Sort Specification Building

The `_build_sort()` static method translates a `Pageable`'s `Sort` orders into pymongo sort tuples:

```python
@staticmethod
def _build_sort(pageable: Pageable) -> list[tuple[str, int]]:
    sort_spec: list[tuple[str, int]] = []
    for order in pageable.sort.orders:
        direction = pymongo.ASCENDING if order.direction == "asc" else pymongo.DESCENDING
        sort_spec.append((order.property, direction))
    return sort_spec
```

For example, `Sort(orders=(Order.desc("created_at"), Order.asc("name")))` becomes:

```python
[("created_at", pymongo.DESCENDING), ("name", pymongo.ASCENDING)]
```

---

## Integration with Web Layer

### Controller with Valid[T] and MongoRepository

The MongoDB adapter integrates seamlessly with PyFly's web layer. Here is a complete example showing a controller that uses `Valid[T]` for request validation and a `MongoRepository` for data access:

```python
# --- Document ---

from pyfly.data.document.mongodb import BaseDocument
from beanie import Indexed
from pydantic import Field


class TaskDocument(BaseDocument):
    title: str
    description: str = ""
    priority: Indexed(int) = 0
    status: str = "TODO"
    assignee: str | None = None

    class Settings:
        name = "tasks"


# --- Repository ---

from pyfly.data.document.mongodb import MongoRepository
from pyfly.container import repository as repo_stereotype


@repo_stereotype
class TaskRepository(MongoRepository[TaskDocument, str]):

    async def find_by_status(self, status: str) -> list[TaskDocument]: ...

    async def find_by_assignee_and_status(
        self, assignee: str, status: str
    ) -> list[TaskDocument]: ...

    async def find_by_priority_greater_than_order_by_priority_desc(
        self, min_priority: int
    ) -> list[TaskDocument]: ...

    async def count_by_status(self, status: str) -> int: ...


# --- Request/Response Models ---

from pydantic import BaseModel


class CreateTaskRequest(BaseModel):
    title: str = Field(..., min_length=1, max_length=500)
    description: str = ""
    priority: int = Field(0, ge=0, le=10)
    assignee: str | None = None


class TaskResponse(BaseModel):
    id: str
    title: str
    description: str
    priority: int
    status: str
    assignee: str | None


# --- Service ---

from pyfly.container import service
from pyfly.data import Mapper


@service
class TaskService:
    def __init__(self, repo: TaskRepository) -> None:
        self._repo = repo
        self._mapper = Mapper()

    async def create(self, request: CreateTaskRequest) -> TaskResponse:
        doc = TaskDocument(
            title=request.title,
            description=request.description,
            priority=request.priority,
            assignee=request.assignee,
        )
        saved = await self._repo.save(doc)
        return TaskResponse(
            id=str(saved.id),
            title=saved.title,
            description=saved.description,
            priority=saved.priority,
            status=saved.status,
            assignee=saved.assignee,
        )

    async def find_by_id(self, task_id: str) -> TaskResponse | None:
        doc = await self._repo.find_by_id(task_id)
        if doc is None:
            return None
        return TaskResponse(
            id=str(doc.id),
            title=doc.title,
            description=doc.description,
            priority=doc.priority,
            status=doc.status,
            assignee=doc.assignee,
        )

    async def find_by_status(self, status: str) -> list[TaskResponse]:
        docs = await self._repo.find_by_status(status)
        return [
            TaskResponse(
                id=str(d.id), title=d.title, description=d.description,
                priority=d.priority, status=d.status, assignee=d.assignee,
            )
            for d in docs
        ]


# --- Controller ---

from pyfly.container import rest_controller
from pyfly.kernel.exceptions import ResourceNotFoundException
from pyfly.web import (
    request_mapping, get_mapping, post_mapping, delete_mapping,
    exception_handler, PathVar, QueryParam, Valid,
)


@rest_controller
@request_mapping("/api/tasks")
class TaskController:
    def __init__(self, task_service: TaskService) -> None:
        self._service = task_service

    @get_mapping("/")
    async def list_tasks(self, status: QueryParam[str] = None) -> list[TaskResponse]:
        if status:
            return await self._service.find_by_status(status)
        return await self._service.find_by_status("TODO")

    @get_mapping("/{task_id}")
    async def get_task(self, task_id: PathVar[str]) -> TaskResponse:
        task = await self._service.find_by_id(task_id)
        if task is None:
            raise ResourceNotFoundException(f"Task {task_id} not found")
        return task

    @post_mapping("/", status_code=201)
    async def create_task(self, body: Valid[CreateTaskRequest]) -> TaskResponse:
        return await self._service.create(body)

    @exception_handler(ResourceNotFoundException)
    async def handle_not_found(self, exc: ResourceNotFoundException):
        return 404, {"error": {"message": str(exc), "code": "TASK_NOT_FOUND"}}
```

---

## Complete CRUD Example

The following example demonstrates a full Product document, repository with derived queries, service, and controller — a complete vertical slice of a PyFly application using MongoDB.

```python
# ==========================================================================
# Document
# ==========================================================================

from pyfly.data.document.mongodb import BaseDocument
from beanie import Indexed, PydanticObjectId
from pydantic import Field


class ProductDocument(BaseDocument):
    """Product stored in the 'products' MongoDB collection."""

    name: str
    sku: Indexed(str, unique=True)
    description: str = ""
    price: float = Field(gt=0)
    category: Indexed(str)
    tags: list[str] = Field(default_factory=list)
    active: bool = True

    class Settings:
        name = "products"


# ==========================================================================
# Repository
# ==========================================================================

from pyfly.data.document.mongodb import MongoRepository
from pyfly.container import repository as repo_stereotype


@repo_stereotype
class ProductRepository(MongoRepository[ProductDocument, PydanticObjectId]):

    # --- Derived query methods (stubs, auto-compiled at startup) ---

    # Equals (default operator)
    async def find_by_category(self, category: str) -> list[ProductDocument]: ...

    # AND connector
    async def find_by_active_and_category(
        self, active: bool, category: str
    ) -> list[ProductDocument]: ...

    # Greater than + ordering
    async def find_by_price_greater_than_order_by_price_desc(
        self, min_price: float
    ) -> list[ProductDocument]: ...

    # Contains (case-insensitive regex)
    async def find_by_name_containing(self, fragment: str) -> list[ProductDocument]: ...

    # Count
    async def count_by_category(self, category: str) -> int: ...

    # Exists
    async def exists_by_sku(self, sku: str) -> bool: ...

    # Delete
    async def delete_by_active(self, active: bool) -> int: ...


# ==========================================================================
# Request / Response Models
# ==========================================================================

from pydantic import BaseModel


class CreateProductRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    sku: str = Field(..., min_length=1, max_length=100)
    description: str = ""
    price: float = Field(..., gt=0)
    category: str
    tags: list[str] = Field(default_factory=list)


class UpdateProductRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: str = ""
    price: float = Field(..., gt=0)
    category: str
    tags: list[str] = Field(default_factory=list)


class ProductResponse(BaseModel):
    id: str
    name: str
    sku: str
    description: str
    price: float
    category: str
    tags: list[str]
    active: bool


# ==========================================================================
# Service
# ==========================================================================

from pyfly.container import service
from pyfly.data import Page, Pageable, Sort
from pyfly.kernel.exceptions import ResourceNotFoundException, ConflictException


@service
class ProductService:
    def __init__(self, repo: ProductRepository) -> None:
        self._repo = repo

    async def create(self, request: CreateProductRequest) -> ProductResponse:
        # Check for duplicate SKU
        if await self._repo.exists_by_sku(request.sku):
            raise ConflictException(
                f"Product with SKU '{request.sku}' already exists",
                code="DUPLICATE_SKU",
            )

        doc = ProductDocument(
            name=request.name,
            sku=request.sku,
            description=request.description,
            price=request.price,
            category=request.category,
            tags=request.tags,
        )
        saved = await self._repo.save(doc)
        return self._to_response(saved)

    async def find_by_id(self, product_id: str) -> ProductResponse:
        doc = await self._repo.find_by_id(product_id)
        if doc is None:
            raise ResourceNotFoundException(
                f"Product {product_id} not found",
                code="PRODUCT_NOT_FOUND",
            )
        return self._to_response(doc)

    async def find_all_active(
        self, category: str | None = None
    ) -> list[ProductResponse]:
        if category:
            docs = await self._repo.find_by_active_and_category(True, category)
        else:
            docs = await self._repo.find_all(active=True)
        return [self._to_response(d) for d in docs]

    async def find_paginated(
        self,
        page: int = 1,
        size: int = 20,
    ) -> Page[ProductResponse]:
        pageable = Pageable.of(
            page=page,
            size=size,
            sort=Sort.by("name"),
        )
        result = await self._repo.find_paginated(pageable=pageable)
        return result.map(self._to_response)

    async def search_by_name(self, query: str) -> list[ProductResponse]:
        docs = await self._repo.find_by_name_containing(query)
        return [self._to_response(d) for d in docs]

    async def delete(self, product_id: str) -> None:
        await self._repo.delete(product_id)

    @staticmethod
    def _to_response(doc: ProductDocument) -> ProductResponse:
        return ProductResponse(
            id=str(doc.id),
            name=doc.name,
            sku=doc.sku,
            description=doc.description,
            price=doc.price,
            category=doc.category,
            tags=doc.tags,
            active=doc.active,
        )


# ==========================================================================
# Controller
# ==========================================================================

from pyfly.container import rest_controller
from pyfly.web import (
    request_mapping, get_mapping, post_mapping, put_mapping, delete_mapping,
    exception_handler, Body, PathVar, QueryParam, Valid,
)


@rest_controller
@request_mapping("/api/products")
class ProductController:

    def __init__(self, product_service: ProductService) -> None:
        self._service = product_service

    @get_mapping("/")
    async def list_products(
        self,
        category: QueryParam[str] = None,
    ) -> list[ProductResponse]:
        """List active products, optionally filtered by category."""
        return await self._service.find_all_active(category=category)

    @get_mapping("/{product_id}")
    async def get_product(self, product_id: PathVar[str]) -> ProductResponse:
        """Get a product by its ID."""
        return await self._service.find_by_id(product_id)

    @get_mapping("/search")
    async def search_products(
        self, q: QueryParam[str] = "",
    ) -> list[ProductResponse]:
        """Search products by name (case-insensitive contains)."""
        return await self._service.search_by_name(q)

    @post_mapping("/", status_code=201)
    async def create_product(self, body: Valid[CreateProductRequest]) -> ProductResponse:
        """Create a new product with Pydantic validation."""
        return await self._service.create(body)

    @delete_mapping("/{product_id}", status_code=204)
    async def delete_product(self, product_id: PathVar[str]) -> None:
        """Delete a product by ID."""
        await self._service.delete(product_id)

    # --- Exception Handlers ---

    @exception_handler(ResourceNotFoundException)
    async def handle_not_found(self, exc: ResourceNotFoundException):
        return 404, {
            "error": {
                "message": str(exc),
                "code": exc.code or "NOT_FOUND",
            }
        }

    @exception_handler(ConflictException)
    async def handle_conflict(self, exc: ConflictException):
        return 409, {
            "error": {
                "message": str(exc),
                "code": exc.code or "CONFLICT",
            }
        }


# ==========================================================================
# Application Bootstrap
# ==========================================================================

from pyfly.core import pyfly_application, PyFlyApplication
from pyfly.web.adapters.starlette import create_app


@pyfly_application(
    name="product-service",
    version="1.0.0",
    scan_packages=["product_service"],
    description="Product catalog microservice backed by MongoDB",
)
class Application:
    pass


async def main():
    pyfly_app = PyFlyApplication(Application)
    await pyfly_app.startup()

    # DocumentAutoConfiguration (discovered via pyfly.auto_configuration entry points)
    # automatically registers and starts:
    #   - motor_client        (AsyncIOMotorClient)
    #   - mongo_post_processor (MongoRepositoryBeanPostProcessor)
    #   - odm_initializer     (BeanieInitializer — scans for BaseDocument subclasses
    #                           and calls init_beanie() during _start_infrastructure())

    # Create the web application
    app = create_app(
        title="Product Catalog",
        version="1.0.0",
        description="CRUD API for product management with MongoDB",
        context=pyfly_app.context,
        docs_enabled=True,
        actuator_enabled=True,
    )

    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

**Configuration file (`pyfly.yaml`):**

```yaml
pyfly:
  app:
    name: product-service
    version: 1.0.0
    description: Product catalog microservice backed by MongoDB

  data:
    document:
      enabled: true
      uri: mongodb://localhost:27017
      database: product_catalog
      min_pool_size: 5
      max_pool_size: 50

  web:
    port: 8080
    docs:
      enabled: true
    actuator:
      enabled: true
```

---

## See Also

- [Data Module Guide](data.md) — Generic commons: repository ports, pagination, query parsing, entity mapping, extensibility
- [Data Relational Guide](data-relational.md) — SQLAlchemy adapter
- [MongoDB Adapter Reference](../adapters/mongodb.md) — Setup, configuration, adapter-specific features
